2022-04-03 13:40:20 [ℹ]  eksctl version 0.90.0
2022-04-03 13:40:20 [ℹ]  using region us-west-2
2022-04-03 13:40:20 [ℹ]  setting availability zones to [us-west-2c us-west-2b us-west-2a]
2022-04-03 13:40:20 [ℹ]  subnets for us-west-2c - public:192.168.0.0/19 private:192.168.96.0/19
2022-04-03 13:40:20 [ℹ]  subnets for us-west-2b - public:192.168.32.0/19 private:192.168.128.0/19
2022-04-03 13:40:20 [ℹ]  subnets for us-west-2a - public:192.168.64.0/19 private:192.168.160.0/19
2022-04-03 13:40:20 [ℹ]  nodegroup "worker-nodes" will use "" [AmazonLinux2/1.21]
2022-04-03 13:40:20 [ℹ]  using Kubernetes version 1.21
2022-04-03 13:40:20 [ℹ]  creating EKS cluster "aws756" in "us-west-2" region with managed nodes
2022-04-03 13:40:20 [ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial managed nodegroup
2022-04-03 13:40:20 [ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-west-2 --cluster=aws756'
2022-04-03 13:40:20 [ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster "aws756" in "us-west-2"
2022-04-03 13:40:20 [ℹ]  CloudWatch logging will not be enabled for cluster "aws756" in "us-west-2"
2022-04-03 13:40:20 [ℹ]  you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=us-west-2 --cluster=aws756'
2022-04-03 13:40:20 [ℹ]  
2 sequential tasks: { create cluster control plane "aws756", 
    2 sequential sub-tasks: { 
        wait for control plane to become ready,
        create managed nodegroup "worker-nodes",
    } 
}
2022-04-03 13:40:20 [ℹ]  building cluster stack "eksctl-aws756-cluster"
2022-04-03 13:40:21 [!]  1 error(s) occurred and cluster hasn't been created properly, you may wish to check CloudFormation console
2022-04-03 13:40:21 [ℹ]  to cleanup resources, run 'eksctl delete cluster --region=us-west-2 --name=aws756'
2022-04-03 13:40:21 [✖]  creating CloudFormation stack "eksctl-aws756-cluster": AlreadyExistsException: Stack [eksctl-aws756-cluster] already exists
	status code: 400, request id: f876ca2d-e9d1-41c1-8d7c-a1db0ff1530a
Renames a context from the kubeconfig file.

 CONTEXT_NAME is the context name that you want to change.

 NEW_NAME is the new name you want to set.

 Note: If the context being renamed is the 'current-context', this field will also be updated.

Examples:
  # Rename the context 'old-name' to 'new-name' in your kubeconfig file
  kubectl config rename-context old-name new-name

Usage:
  kubectl config rename-context CONTEXT_NAME NEW_NAME [options]

Use "kubectl options" for a list of global command-line options (applies to all commands).
